{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,confusion_matrix)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4292bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067109b",
   "metadata": {},
   "source": [
    "### Loading and Splitting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(path):\n",
    "    # load data from CSV file\n",
    "    df = pd.read_csv(path)\n",
    "    # extract text and label columns\n",
    "    texts = df['review']\n",
    "    labels = df['sentiment']\n",
    "    # map 'positive' and 'negative' labels to 1 and 0, respectively\n",
    "    labels = labels.map({'positive': 1, 'negative': 0})\n",
    "    # return DataFrame with text and label columns\n",
    "    return pd.DataFrame({'text': texts, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset using the function we defined earlier\n",
    "data = load_imdb_data('IMDB Dataset.csv')\n",
    "# split the data into train and test sets\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# reset the index for both train and test DataFrames\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3ebd2",
   "metadata": {},
   "source": [
    "### preprocessing \n",
    "\n",
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b13d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower() \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters, punctuation, and emojis\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'_+', '', text)\n",
    "    text = re.sub(r'<br\\s*\\/?>', '', text)\n",
    "    text = text.replace('br', '')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove extra white spaces, tabs, and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove language-specific stopwords\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    # Stemming\n",
    "    words = [wnl.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning train and test data\n",
    "train['text'] = train['text'].apply(lambda x: clean(x))\n",
    "test['text'] = test['text'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5e48c",
   "metadata": {},
   "source": [
    "### word vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the text into a sparse matrix of token counts using the CountVectorizer object\n",
    "vectorizer = CountVectorizer(max_features=10000, ngram_range=(2, 2))\n",
    "X_train_vectors = vectorizer.fit_transform(train['text'])\n",
    "X_train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first row of the sparse matrix of token counts\n",
    "X_train_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27022e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d146",
   "metadata": {},
   "source": [
    "#### Bi-grams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    ngrams = list(nltk.ngrams(tokens, n))\n",
    "    return [' '.join(gram) for gram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bigrams'] = train['text'].apply(lambda x: generate_ngrams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the polarity scoring function to each bigram \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "train['bigram_sentiment'] = train['bigrams'].apply(lambda x: [analyzer.polarity_scores(gram)['compound'] for gram in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out only the positive and negative bigrams  \n",
    "positive_bigrams = train.explode('bigram_sentiment')[train['label'] == 1]['bigram_sentiment']\n",
    "negative_bigrams = train.explode('bigram_sentiment')[train['label'] == 0]['bigram_sentiment']\n",
    "#Calculate the mean sentiment score for each bigram\n",
    "positive_bigrams_mean = positive_bigrams.groupby(positive_bigrams.index).mean()\n",
    "negative_bigrams_mean = negative_bigrams.groupby(negative_bigrams.index).mean()\n",
    "#Print the top 10 positive and negative bigrams, sorted by mean sentiment score.\n",
    "print('Top 10 positive bigrams:')\n",
    "print(positive_bigrams_mean.sort_values(ascending=False)[:10])\n",
    "\n",
    "print('Top 10 negative bigrams:')\n",
    "print(negative_bigrams_mean.sort_values(ascending=True)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d112b",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the n-gram range and maximum number of features\n",
    "ngram_range = (1, 2)\n",
    "max_features = 10000\n",
    "\n",
    "# instantiate the count vectorizer with the specified n-gram range and maximum number of features\n",
    "vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
    "\n",
    "# fit and transform the training data\n",
    "X_train_vectors = vectorizer.fit_transform(train['text'])\n",
    "\n",
    "# get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# get the top occurring n-grams for positive and negative reviews\n",
    "top_ngrams_pos = pd.Series(X_train_vectors[train['label']==1].sum(axis=0).A1, index=feature_names).sort_values(ascending=False)[:10]\n",
    "top_ngrams_neg = pd.Series(X_train_vectors[train['label']==0].sum(axis=0).A1, index=feature_names).sort_values(ascending=False)[:10]\n",
    "\n",
    "# plot the bar chart for top occurring n-grams in positive and negative reviews\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].barh(top_ngrams_pos.index, top_ngrams_pos.values, color='green')\n",
    "ax[0].set_title('Top 10 N-grams in Positive Reviews')\n",
    "ax[1].barh(top_ngrams_neg.index, top_ngrams_neg.values, color='red')\n",
    "ax[1].set_title('Top 10 N-grams in Negative Reviews')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14ef71",
   "metadata": {},
   "source": [
    "#### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = ' '.join(train['text'])\n",
    "\n",
    "# split the string into individual words\n",
    "all_words = all_reviews.split()\n",
    "\n",
    "# count the frequency of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# create a dataframe of the top 20 most common words\n",
    "top_words = pd.DataFrame(word_counts.most_common(20), columns=['word', 'count'])\n",
    "\n",
    "# plot the word frequency distribution\n",
    "top_words.plot(kind='bar', x='word')\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f4373",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Training using our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e507965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize classifier and Fit the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectors, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on training data\n",
    "model.score(X_train_vectors, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2b33b",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the test data into vectors\n",
    "X_test_vectors = vectorizer.transform(test['text'])\n",
    "X_test_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4756c19",
   "metadata": {},
   "source": [
    "#### evaluate classifier on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40875f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the labels of the test data\n",
    "y_test_hat = model.predict(X_test_vectors)\n",
    "y_test_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f1f28",
   "metadata": {},
   "source": [
    "#### accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on test data\n",
    "accuracy_score(test.label, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba11d0",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb45ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report on test data\n",
    "print(classification_report(test['label'], y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ee5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test.label, y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d363d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix using a heatmap\n",
    "sns.heatmap(pd.DataFrame(confusion_matrix(test.label, y_test_hat)), annot=True, cmap=\"Purples\", fmt=\"d\", cbar=False, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
